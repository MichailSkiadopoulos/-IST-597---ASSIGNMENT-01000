{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PRE Batch Normalization on FMNIST with MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVXrs05QAm6x"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "np.random.seed(3*9755)\n",
        "tf.random.set_seed(3*9755)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.config.list_physical_devices('GPU')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NdxAY-m6GS_u",
        "outputId": "1460b060-e9c6-487f-932a-9b23c52b28d8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train, y_train), (X_test, y_test) = keras.datasets.fashion_mnist.load_data() # Load MNIST or FMNIST\n",
        "assert X_train.shape == (60000, 28, 28)\n",
        "assert X_test.shape == (10000, 28, 28)\n",
        "assert y_train.shape == (60000,)\n",
        "assert y_test.shape == (10000,)"
      ],
      "metadata": {
        "id": "e5U8Yf8iGUc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train dataset into train and validation\n",
        "X_val = X_train[50000:60000]\n",
        "X_train = X_train[0:50000]\n",
        "y_val = y_train[50000:60000]\n",
        "y_train = y_train[0:50000]\n",
        "\n",
        "X_train = X_train.reshape(50000, 28*28)\n",
        "X_val = X_val.reshape(10000, 28*28)\n",
        "X_test = X_test.reshape(10000, 28*28)"
      ],
      "metadata": {
        "id": "4IWXruviGWfx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize Data\n",
        "\n",
        "X_train = X_train/255\n",
        "X_val = X_val/255\n",
        "X_test = X_test/255\n",
        "# X_train[0]\n",
        "np.max(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VRvI52hYGaDh",
        "outputId": "20e2c4ec-9580-47a3-d8d3-bc95ec6193be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "size_input = X_train.shape[1]\n",
        "size_hidden1 = 128\n",
        "size_hidden2 = 128\n",
        "size_hidden3 = 128\n",
        "size_output = 10\n",
        "\n",
        "number_of_train_examples = X_train.shape[0]\n",
        "number_of_test_examples = X_test.shape[0]\n",
        "\n",
        "y_train = tf.keras.utils.to_categorical(y_train, num_classes=10) # Other function is tf.one_hot(y_train,depth=10)\n",
        "y_val = tf.keras.utils.to_categorical(y_val, num_classes=10)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
        "print(tf.shape(y_val))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b_FmerWQGc4-",
        "outputId": "89c48394-e054-40af-c75c-d0abe82eefab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([10000    10], shape=(2,), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define class to build mlp model\n",
        "class MLP(object):\n",
        " def __init__(self, size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device=None):\n",
        "    \"\"\"\n",
        "    size_input: int, size of input layer\n",
        "    size_hidden1: int, size of the 1st hidden layer\n",
        "    size_hidden2: int, size of the 2nd hidden layer\n",
        "    size_output: int, size of output layer\n",
        "    device: str or None, either 'cpu' or 'gpu' or None. If None, the device to be used will be decided automatically during Eager Execution\n",
        "    \"\"\"\n",
        "    self.size_input, self.size_hidden1, self.size_hidden2, self.size_hidden3, self.size_output, self.device =\\\n",
        "    size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device\n",
        "    \n",
        "    # Initialize weights between input mapping and a layer g(f(x)) = layer\n",
        "    self.W1 = tf.Variable(tf.random.normal([self.size_input, self.size_hidden1],stddev=0.1)) # Xavier(Fan-in fan-out) and Orthogonal\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b1 = tf.Variable(tf.zeros([1, self.size_hidden1])) # 0 or constant(0.01)\n",
        "    \n",
        "    # Initialize weights between input layer and 1st hidden layer\n",
        "    self.W2 = tf.Variable(tf.random.normal([self.size_hidden1, self.size_hidden2],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b2 = tf.Variable(tf.zeros([1, self.size_hidden2]))\n",
        "    \n",
        "    # Initialize weights between 1st hidden layer and 2nd hidden layer\n",
        "    self.W3 = tf.Variable(tf.random.normal([self.size_hidden2, self.size_hidden3],stddev=0.1))\n",
        "    # Initialize biases for hidden layer\n",
        "    self.b3 = tf.Variable(tf.zeros([1, self.size_hidden3]))\n",
        "    \n",
        "     # Initialize weights between 2nd hidden layer and output layer\n",
        "    self.W4 = tf.Variable(tf.random.normal([self.size_hidden3, self.size_output],stddev=0.1))\n",
        "    # Initialize biases for output layer\n",
        "    self.b4 = tf.Variable(tf.zeros([1, self.size_output]))\n",
        "    \n",
        "    self.gamma = tf.Variable(1, dtype=tf.float32)\n",
        "    self.beta = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "    # Define variables to be updated during backpropagation\n",
        "    self.variables = [self.W1, self.W2, self.W3, self.W4, self.b1, self.b2, self.b3, self.b4, self.gamma, self.beta]\n",
        "  \n",
        " def forward(self, X, X_test, function):\n",
        "    \"\"\"\n",
        "    forward pass\n",
        "    X: Tensor, inputs\n",
        "    \"\"\"\n",
        "    if self.device is not None:\n",
        "      with tf.device('gpu:0' if self.device=='gpu' else 'cpu'):\n",
        "        self.y = self.compute_output(X, X_test,function)\n",
        "    else:\n",
        "      self.y = self.compute_output(X, X_test, function)\n",
        "      \n",
        "    return self.y\n",
        "\n",
        " def loss(self, y_pred, y_true):\n",
        "    '''\n",
        "    y_pred - Tensor of shape (batch_size, size_output)\n",
        "    y_true - Tensor of shape (batch_size, size_output)\n",
        "    '''\n",
        "    #y_true_tf = tf.cast(tf.reshape(y_true, (-1, self.size_output)), dtype=tf.float32)\n",
        "    y_true_tf = tf.cast(y_true, dtype=tf.float32)\n",
        "    y_pred_tf = tf.cast(y_pred, dtype=tf.float32)\n",
        "    cce = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
        "    loss_x = cce(y_true_tf, y_pred_tf)\n",
        "    # Use keras or tf_softmax, both should work for any given model\n",
        "    #loss_x = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_pred_tf, labels=y_true_tf))\n",
        "    \n",
        "    return loss_x\n",
        "\n",
        " def backward(self, X_train, X_test, y_train, opti, function):\n",
        "    \"\"\"\n",
        "    backward pass\n",
        "    \"\"\"\n",
        "    optimizer = opti\n",
        "    \n",
        "    with tf.GradientTape() as tape:\n",
        "        \n",
        "      predicted = self.forward(X_train, X_test, function)\n",
        "      current_loss = self.loss(predicted, y_train)\n",
        "        \n",
        "    grads = tape.gradient(current_loss, self.variables)\n",
        "    optimizer.apply_gradients(zip(grads, self.variables))\n",
        "\n",
        " def BN_train(self, X, gamma, beta, eps):\n",
        "\n",
        "    mean = np.mean(X)\n",
        "    var = np.mean(((X - mean) ** 2))\n",
        "\n",
        "    X_hat = (X - mean) / np.sqrt(var + eps)\n",
        "    BN_Out = gamma * X_hat + beta\n",
        "\n",
        "    return BN_Out\n",
        "\n",
        " def BN_test(self, X, X_test, gamma, beta, eps):\n",
        "\n",
        "    mean = np.mean(X_test)\n",
        "    var = np.mean(((X_test - mean) ** 2))\n",
        "\n",
        "    X_hat = (X - mean) / np.sqrt(var + eps)\n",
        "    BN_Out = gamma * X_hat + beta\n",
        "\n",
        "    return BN_Out\n",
        "           \n",
        " def compute_output(self, X, Xtest, function):\n",
        "    \"\"\"\n",
        "    Custom method to obtain output tensor during forward pass\n",
        "    \"\"\"\n",
        "    eps=1e-12\n",
        "    gamma=self.variables[-2]\n",
        "    beta=self.variables[-1]\n",
        "\n",
        "    # Cast X to float32\n",
        "    X_tf = tf.cast(X, dtype=tf.float32)\n",
        "    X_test_tf = tf.cast(X_test, dtype=tf.float32)\n",
        "    #X_tf = X\n",
        "    \n",
        "    # Compute values in hidden layers\n",
        "    if function == 'train':\n",
        "      z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "      z1 = self.BN_train(z1, gamma, beta, eps)\n",
        "      h1 = tf.nn.relu(z1)\n",
        "\n",
        "      z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "      z2 = self.BN_train(z2, gamma, beta, eps)\n",
        "      h2 = tf.nn.relu(z2)\n",
        "\n",
        "      z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "      z3 = self.BN_train(z3, gamma, beta, eps)\n",
        "      h3 = tf.nn.relu(z3)\n",
        "    elif function  == 'test':\n",
        "      z1 = tf.matmul(X_tf, self.W1) + self.b1\n",
        "      z1_test = tf.matmul(X_test_tf, self.W1) + self.b1\n",
        "      h1_test = tf.nn.relu(z1_test)\n",
        "\n",
        "      z1 = self.BN_test(z1, z1_test, gamma, beta, eps)\n",
        "      h1 = tf.nn.relu(z1)\n",
        "\n",
        "      z2 = tf.matmul(h1, self.W2) + self.b2\n",
        "      z2_test = tf.matmul(h1_test, self.W2) + self.b2\n",
        "      h2_test = tf.nn.relu(z2_test)\n",
        "      \n",
        "      z2 = self.BN_test(z2, z2_test, gamma, beta, eps)\n",
        "      h2 = tf.nn.relu(z2)\n",
        "\n",
        "      z3 = tf.matmul(h2, self.W3) + self.b3\n",
        "      z3_test = tf.matmul(h2_test, self.W3) + self.b3\n",
        "      h3_test = tf.nn.relu(z3_test)\n",
        "      \n",
        "      z3 = self.BN_test(z3, z3_test, gamma, beta, eps)\n",
        "      h3 = tf.nn.relu(z3)\n",
        "\n",
        "    # Compute output\n",
        "    output = tf.matmul(h3, self.W4) + self.b4\n",
        "    return output;\n"
      ],
      "metadata": {
        "id": "_RvcwVX2GgJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set number of epochs\n",
        "NUM_EPOCHS = 10\n",
        "\n",
        "# Initialize model using CPU\n",
        "mlp_on_cpu = MLP(size_input, size_hidden1, size_hidden2, size_hidden3, size_output, device='gpu')\n",
        "va = 0\n",
        "time_start = time.time()\n",
        "opti = tf.keras.optimizers.SGD(learning_rate = 0.1)\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    \n",
        "  loss_total = tf.zeros([1,1], dtype=tf.float32)\n",
        "  lt = 0\n",
        "    \n",
        "  train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(25, seed=epoch*(3*9755)).batch(128)\n",
        "  kz = 0\n",
        "  accuracy_z = 0.0\n",
        "  cur_train_acc = 0.0\n",
        "  for inputs, outputs in train_ds:\n",
        "    qw, tr = tf.shape(inputs)\n",
        "    kz = kz + 1\n",
        "    preds = mlp_on_cpu.forward(inputs,X_train, 'train') \n",
        "    loss_total = loss_total + mlp_on_cpu.loss(preds, outputs)\n",
        "    lt = lt + mlp_on_cpu.loss(preds, outputs)\n",
        "    mlp_on_cpu.backward(inputs, X_train, outputs, opti, 'train')\n",
        "\n",
        "  preds = mlp_on_cpu.forward(X_train, X_train, 'train')\n",
        "  # Get probs, remember we only have logits from our forward function, we need to apply softmax on top of it to get probs\n",
        "  preds = tf.nn.softmax(preds)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds, 1), tf.argmax(y_train, 1))\n",
        "  accuracy_z = accuracy_z + tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_train_acc += accuracy_z.numpy()\n",
        "  ds = cur_train_acc\n",
        "  print('\\nTrain Accuracy: {:.4f}'.format(ds))\n",
        "  print('Number of Epoch = {} - Average Cross Entropy:= {} '.format(epoch + 1, np.sum(loss_total) / X_train.shape[0]))\n",
        "    \n",
        "  preds_val = mlp_on_cpu.forward(X_val, X_val, 'train')\n",
        "  preds_val = tf.nn.softmax(preds_val)\n",
        "  correct_prediction = tf.equal(tf.argmax(preds_val, 1), tf.argmax(y_val, 1))\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "  cur_val_acc = accuracy.numpy()\n",
        "  if va > cur_val_acc:\n",
        "    break\n",
        "  va = cur_val_acc\n",
        "  print('\\nValidation Accuracy: {:.4f}'.format(cur_val_acc))\n",
        "  \n",
        "  plt.plot(epoch + 1, np.sum(loss_total) / X_train.shape[0], 'go')\n",
        "\n",
        "        \n",
        "time_taken = time.time() - time_start\n",
        "    \n",
        "# Validate model\n",
        "    \n",
        "\n",
        "\n",
        "print('\\nTotal time taken (in seconds): {:.2f}'.format(time_taken))\n",
        "#For per epoch_time = Total_Time / Number_of_epochs\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 616
        },
        "id": "7bRZPjlKGpDJ",
        "outputId": "085a7d8b-de02-4a5f-9f77-1cb0a77dd70e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train Accuracy: 0.8424\n",
            "Number of Epoch = 1 - Average Cross Entropy:= 0.004904132690429688 \n",
            "\n",
            "Validation Accuracy: 0.8317\n",
            "\n",
            "Train Accuracy: 0.8467\n",
            "Number of Epoch = 2 - Average Cross Entropy:= 0.0033044778442382814 \n",
            "\n",
            "Validation Accuracy: 0.8326\n",
            "\n",
            "Train Accuracy: 0.8759\n",
            "Number of Epoch = 3 - Average Cross Entropy:= 0.0029632772827148436 \n",
            "\n",
            "Validation Accuracy: 0.8595\n",
            "\n",
            "Train Accuracy: 0.8732\n",
            "Number of Epoch = 4 - Average Cross Entropy:= 0.002707757568359375 \n",
            "\n",
            "Total time taken (in seconds): 152.97\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize\n",
        "test_loss_total = tf.Variable(0, dtype=tf.float32)\n",
        "correct_prediction = tf.Variable(0, dtype=tf.float32)\n",
        "\n",
        "# Test model\n",
        "preds_test = mlp_on_cpu.forward(X_test, X_test, 'train')\n",
        "preds_test = tf.nn.softmax(preds_test)\n",
        "correct_prediction = tf.equal(tf.argmax(preds_test, 1), tf.argmax(y_test, 1))\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
        "cur_test_acc = accuracy.numpy()\n",
        "print('\\nTest Accuracy: {:.2f}'.format(cur_test_acc))"
      ],
      "metadata": {
        "id": "-PHtAakvfHEC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c9dc49-1035-4e38-bf3b-8cff92c04363"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Test Accuracy: 0.85\n"
          ]
        }
      ]
    }
  ]
}